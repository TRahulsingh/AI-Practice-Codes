{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gAj13mtMLNl7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class TicTacToe:\n",
        "    def __init__(self, state=None):\n",
        "        self.board = np.zeros((3, 3)) if state is None else np.array(state)\n",
        "        self.players = [\"X\", \"O\"]\n",
        "        self.current_player = self.players[0]\n",
        "        self.winner = None\n",
        "        self.game_over = False\n",
        "\n",
        "    def make_move(self, move):\n",
        "        if self.board[move[0]][move[1]] != 0:\n",
        "            return self.board, -10, self.game_over  # Invalid move penalty\n",
        "        self.board[move[0]][move[1]] = self.players.index(self.current_player) + 1\n",
        "        self.check_winner()\n",
        "        reward = 1 if self.winner else 0\n",
        "        self.switch_player()\n",
        "        return self.board, reward, self.game_over\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((3, 3))\n",
        "        self.current_player = self.players[0]\n",
        "        self.winner = None\n",
        "        self.game_over = False\n",
        "\n",
        "    def available_moves(self):\n",
        "        return [(i, j) for i in range(3) for j in range(3) if self.board[i][j] == 0]\n",
        "\n",
        "    def switch_player(self):\n",
        "        self.current_player = self.players[1] if self.current_player == self.players[0] else self.players[0]\n",
        "\n",
        "    def check_winner(self):\n",
        "        for i in range(3):\n",
        "            if self.board[i][0] == self.board[i][1] == self.board[i][2] != 0:\n",
        "                self.winner = self.players[int(self.board[i][0] - 1)]\n",
        "                self.game_over = True\n",
        "        for j in range(3):\n",
        "            if self.board[0][j] == self.board[1][j] == self.board[2][j] != 0:\n",
        "                self.winner = self.players[int(self.board[0][j] - 1)]\n",
        "                self.game_over = True\n",
        "        if self.board[0][0] == self.board[1][1] == self.board[2][2] != 0:\n",
        "            self.winner = self.players[int(self.board[0][0] - 1)]\n",
        "            self.game_over = True\n",
        "        if self.board[0][2] == self.board[1][1] == self.board[2][0] != 0:\n",
        "            self.winner = self.players[int(self.board[0][2] - 1)]\n",
        "            self.game_over = True\n",
        "        if not any(0 in row for row in self.board):\n",
        "            self.game_over = True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, alpha, epsilon, discount_factor):\n",
        "        self.Q = {}\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.discount_factor = discount_factor\n",
        "\n",
        "    def get_Q_value(self, state, action):\n",
        "        if (tuple(state.flatten()), action) not in self.Q:\n",
        "            self.Q[(tuple(state.flatten()), action)] = 0.0\n",
        "        return self.Q[(tuple(state.flatten()), action)]\n",
        "\n",
        "    def choose_action(self, state, available_moves):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(available_moves)\n",
        "        else:\n",
        "            Q_values = [self.get_Q_value(state, action) for action in available_moves]\n",
        "            max_Q = max(Q_values)\n",
        "            best_moves = [action for action, Q_value in zip(available_moves, Q_values) if Q_value == max_Q]\n",
        "            return random.choice(best_moves)\n",
        "\n",
        "    def update_Q_value(self, state, action, reward, next_state, next_available_moves):\n",
        "        current_Q = self.get_Q_value(state, action)\n",
        "        if next_available_moves:\n",
        "            next_Q = max([self.get_Q_value(next_state, next_action) for next_action in next_available_moves])\n",
        "        else:\n",
        "            next_Q = 0\n",
        "        self.Q[(tuple(state.flatten()), action)] = current_Q + self.alpha * (reward + self.discount_factor * next_Q - current_Q)\n"
      ],
      "metadata": {
        "id": "EnfOad8bLQH3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_episodes, alpha, epsilon, discount_factor):\n",
        "    agent = QLearningAgent(alpha, epsilon, discount_factor)\n",
        "    for _ in range(num_episodes):\n",
        "        game = TicTacToe()\n",
        "        state = game.board\n",
        "        while not game.game_over:\n",
        "            available_moves = game.available_moves()\n",
        "            action = agent.choose_action(state, available_moves)\n",
        "            next_state, reward, game_over = game.make_move(action)\n",
        "            agent.update_Q_value(state, action, reward, next_state, game.available_moves())\n",
        "            state = next_state\n",
        "    return agent\n"
      ],
      "metadata": {
        "id": "EQffk0u3LSiJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(agent, num_games):\n",
        "    num_wins = 0\n",
        "    num_draws = 0\n",
        "    num_losses = 0\n",
        "\n",
        "    for _ in range(num_games):\n",
        "        game = TicTacToe()\n",
        "        state = game.board\n",
        "        while not game.game_over:\n",
        "            if game.current_player == \"X\":  # Assume agent is always \"X\"\n",
        "                action = agent.choose_action(state, game.available_moves())\n",
        "            else:\n",
        "                action = random.choice(game.available_moves())  # Random opponent\n",
        "\n",
        "            state, reward, game_over = game.make_move(action)\n",
        "\n",
        "        if game.winner == \"X\":  # Agent wins\n",
        "            num_wins += 1\n",
        "        elif game.winner == \"O\":  # Agent loses\n",
        "            num_losses += 1\n",
        "        else:  # No winner (draw)\n",
        "            num_draws += 1\n",
        "\n",
        "    win_rate = num_wins / num_games * 100\n",
        "    draw_rate = num_draws / num_games * 100\n",
        "    loss_rate = num_losses / num_games * 100\n",
        "\n",
        "    return win_rate, draw_rate, loss_rate\n"
      ],
      "metadata": {
        "id": "CHzLofYFLUkY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "num_episodes = 100000\n",
        "alpha = 0.5\n",
        "epsilon = 0.1\n",
        "discount_factor = 1.0\n",
        "num_games = 1000\n",
        "\n",
        "# Train the Q-learning agent\n",
        "agent = train(num_episodes, alpha, epsilon, discount_factor)\n",
        "\n",
        "# Test the Q-learning agent\n",
        "win_rate, draw_rate, loss_rate = test(agent, num_games)\n",
        "\n",
        "# Display results\n",
        "print(f\"Win Rate: {win_rate:.2f}%\")\n",
        "print(f\"Draw Rate: {draw_rate:.2f}%\")\n",
        "print(f\"Loss Rate: {loss_rate:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEhMoQ9lLWgQ",
        "outputId": "8ac64294-5680-414e-ad8b-0a708fcb067a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Win Rate: 59.50%\n",
            "Draw Rate: 13.70%\n",
            "Loss Rate: 26.80%\n"
          ]
        }
      ]
    }
  ]
}